{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harry's House\n",
    "\n",
    "Il nostro obiettivo e' di raccogliere informazioni sull'album in uscita il 20 Maggio 2022. Le domande a cui cercheremo di rispondere sono le sequenti:\n",
    "1. _Quali sono i paesi in cui si è più discusso dell’album? Corrispondono ai paesi in cui l’album o le canzoni sono in vetta alle classifiche? => **Analisi delle publics metrics confrontate con i dati di Spotify**_ \n",
    "2. _Durante il giorno dell’uscita dell’album, l’hashtag #harryhouse è stato in tendenza? Anche nei giorni precedenti e successivi? Se si per quanto tempo?_\n",
    "3. _Le canzoni più citate hanno per caso impattato altri social? Sono diventati brani in tendenza su altri social (tiktok)? => **Analisi della distribuzione degli hashtag e del trand**_\n",
    "4. _Dalle reti delle menzioni di Twitter è possibile estrarre un buon modo per stilare una classifica simile se non uguale a quella di Spotify? ⇒ **Review sugli indici di centralità**_\n",
    "5. _Quale è l’andamento delle varie canzoni dopo l’uscita dell’album? Il trend delle citazioni è simile al trend delle canzoni su Spotify (classifica globale)?_\n",
    "6. _Quale è il sentimento percepito dai follower prima dell’uscita, il giorno dell’uscita e dopo?  ⇒ **Sentiment Analysis**_\n",
    "7. _Quale è la tipologia di utenza? E’ cambiata durante i giorni antecedenti e post all’uscita dell’album? => **Analisi delle community**_\n",
    "8. _è possibile identificare community di utenti, outliers, che si sono aggiunti alle discussioni? ⇒ **User Clustering**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dei dati necessari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniziamo a richiedere i dati necessari alla nostra analisi, i dati raccolti comprendono un arco di tempo che va dal 10 Maggio 2022 fino al (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT\n",
    "\n",
    "import tweepy\n",
    "import CONFIG\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarac\\OneDrive - Università degli Studi di Milano\\UNI 2022\\_SOCIALMEDIA\\#harryhouse\\HARRY_HOUSE_DATA\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd() + '\\HARRY_HOUSE_DATA'\n",
    "os.chdir(path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token = CONFIG.BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_harryshouse = \"HarrysHouse -is:retweet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chiamata per prendere gli id dei tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9324\n"
     ]
    }
   ],
   "source": [
    "list_tweet_id = []\n",
    "\n",
    "for t in tweepy.Paginator(\n",
    "    client.search_recent_tweets,\n",
    "    query = query_harryshouse,\n",
    "    max_results = 100,\n",
    "    \n",
    ").flatten():\n",
    "    list_tweet_id.append(t.id)\n",
    "\n",
    "print(len(list_tweet_id))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(list_tweet_id, open('tweets_id_1705.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tweet = json.load(open('tweets_id_1705.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estensione dei campi dei tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(list_tweet)/100\n",
    "max_richieste = int(np.ceil(n))\n",
    "max_richieste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14576/3830446260.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0msub_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_tweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mmin_valore\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     response = client.get_tweets(\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msub_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         tweet_fields = [\n",
      "\u001b[1;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "tweets_extended = []\n",
    "count = 0\n",
    "\n",
    "for i in range(max_richieste):\n",
    "    count += 1\n",
    "\n",
    "    min_valore = min((i+1)*100, len(list_tweet))\n",
    "    sub_list = list_tweet[(i*100) : min_valore]\n",
    "    \n",
    "    response = client.get_tweets(\n",
    "        ids = sub_list,\n",
    "        tweet_fields = [\n",
    "            'created_at', \n",
    "            'lang', \n",
    "            'entities', \n",
    "            'geo', \n",
    "            'public_metrics'],\n",
    "        expansions = ['author_id', 'geo.place_id'],\n",
    "        user_fields = ['location', \n",
    "                       'verified', \n",
    "                       'public_metrics', \n",
    "                       'id', \n",
    "                       'username',\n",
    "                       'name'\n",
    "                      ]\n",
    "    )\n",
    "    \n",
    "    users = {u.data['id']: u.data for u in response.includes['users']}\n",
    "\n",
    "    for t in response.data:\n",
    "        tweet_dict = t.data\n",
    "        tweet_dict['author'] = users[tweet_dict['author_id']]\n",
    "        tweets_extended.append(tweet_dict)\n",
    "    \n",
    "        #INFO TWEET\n",
    "    \n",
    "        #print(\"tweet id - \", t.id)\n",
    "        print(\"lang - \", t.lang)\n",
    "        print(\"date creation - \", t.created_at)\n",
    "        print(\"entities: \", t.entities)\n",
    "        print(\"public metrics: \", t.public_metrics)\n",
    "        if t.geo:\n",
    "            print(\"geo: \", t.geo)\n",
    "    \n",
    "    \n",
    "        #INFO USER\n",
    "\n",
    "        if users[t.data['author_id']]:\n",
    "            id_author = t.data['author_id']\n",
    "            author = users[id_author]\n",
    "            \n",
    "            \n",
    "            print(\"username author: \", author['username'])\n",
    "            print(\"verified: \", author['verified'])\n",
    "\n",
    "\n",
    "            print(\"public_metrics\")\n",
    "            author = user['public_metrics']\n",
    "            print(\"- follower count: \", u['followers_count'])\n",
    "            print(\"- following count: \", u['following_count'])\n",
    "            print(\"- tweet count: \", u['tweet_count'])\n",
    "            \n",
    "            try:\n",
    "                location = author['location']\n",
    "            except:\n",
    "                location = \"\"\n",
    "             \n",
    "            print(\"location: \" + location)\n",
    "\n",
    "        #INFO PLACES\n",
    "        \n",
    "        try:\n",
    "            place = places[t.geo['place_id']]\n",
    "        except:\n",
    "            place = \"\"\n",
    "            \n",
    "        print(\"place: \", place)\n",
    "        if t.geo['place_id']:\n",
    "            place = places[t.geo['place_id']]\n",
    "            print(place.full_name)\n",
    "    \n",
    "   \n",
    "        print(\"---------\")\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME\n",
    "\n",
    "json.dump(tweets_extended, open('tweets_extended_1705.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME\n",
    "\n",
    "tweets_extended = json.load(open('tweets_extended_1705.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi del contenuto ottenuto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = []\n",
    "\n",
    "for t in tweets_extended:\n",
    "    try:\n",
    "        tweets = t['entities'].get('hashtags')\n",
    "        for x in tweets:\n",
    "            hashtags.append(x['tag'])\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17753\n"
     ]
    }
   ],
   "source": [
    "print(len(hashtags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a contare i dati prodotti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hashtag</th>\n",
       "      <th>Tot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HarrysHouse</td>\n",
       "      <td>8919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HarryStyles</td>\n",
       "      <td>2092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AsItWas</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>harrystyles</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>harryshouse</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HarryStylesAvecNRJ</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>harrystylesONO</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ONOTICKETS</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OneNightOnly</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>YouAreHome</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>harrystylestickets</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LoveOnTour2022</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HarrysHousePH</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hs3</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ono</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HarryStylesLoveOnTour</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HSIQ</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ONO</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>loveontour</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>HS3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Hashtag   Tot\n",
       "0             HarrysHouse  8919\n",
       "1             HarryStyles  2092\n",
       "2                 AsItWas   414\n",
       "3             harrystyles   405\n",
       "4             harryshouse   301\n",
       "5      HarryStylesAvecNRJ   207\n",
       "6          harrystylesONO   172\n",
       "7              ONOTICKETS   158\n",
       "8            OneNightOnly   123\n",
       "9              YouAreHome   114\n",
       "10     harrystylestickets   110\n",
       "11         LoveOnTour2022    87\n",
       "12          HarrysHousePH    85\n",
       "13                    hs3    67\n",
       "14                    ono    64\n",
       "15  HarryStylesLoveOnTour    62\n",
       "16                   HSIQ    59\n",
       "17                    ONO    56\n",
       "18             loveontour    55\n",
       "19                    HS3    52"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_hashtags = Counter(hashtags)\n",
    "\n",
    "pd.DataFrame(\n",
    "    c_hashtags.most_common()[:20],\n",
    "    columns = ['Hashtag', 'Tot']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esportiamo tutto in un file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = [\"Hashtag\", \"Tot\"]\n",
    "rows = []\n",
    "for v in  c_hashtags.most_common()[:20]:\n",
    "    row = []\n",
    "    row.append(v[0])\n",
    "    row.append(str(v[1]))\n",
    "    rows.append(row)\n",
    "    \n",
    "with open('tot_hashtags_1705.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(head)\n",
    "    writer.writerows(rows)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esportiamo tutto su un file json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HarrysHouse': 8919, 'HarryStyles': 2092, 'harrystylesono': 41, 'onenightonly': 14, 'ONOTICKETS': 158, 'harrystyles': 405, 'AsItWas': 414, 'TodayShow': 6, 'fanpasses': 2, 'NYC': 4, 'hshq': 13, 'LoveOnTour2022': 87, 'onoharrystyles': 48, 'RegístrateGratis': 3, 'YouAreHome': 114, 'HARRYCHELLA': 27, 'Harrystylesconcert': 16, 'HarryStylesLoveOnTour2022': 19, 'onoroff': 2, 'ONO': 56, 'ENGLAND': 2, 'listeningparty': 16, 'OneNightOnly': 123, 'harry': 33, 'harrystylesONO': 172, 'harrystylestickets': 110, 'onenightonlytickets': 5, 'harrytickets': 7, 'AmberHeard': 2, 'cryptocurrency': 5, 'DeppVsHeard': 11, 'ElonMusk': 2, 'GMT': 2, 'HarshadChopda': 2, 'JohnnyDepp': 4, 'LTWTSantiago': 3, 'MoveToEarn': 2, 'fineline': 22, 'harryshouse': 301, 'loveontour': 55, 'hslot': 24, 'Coachella': 10, 'ONOlondon': 3, 'loveontourparis': 2, 'DontWorryDarling': 12, 'mypoliceman': 8, 'FreeHarry': 12, 'TreatPeopleWithKindness': 5, 'ChooseLove': 9, 'ono': 64, 'NY': 6, 'harries': 25, 'HarryStylesOneNightOnly': 6, 'tickets': 14, 'freeharry': 4, 'freebritney': 3, 'HSLOT': 31, 'Casper': 2, 'scammer': 4, 'onotickets': 16, 'summer': 4, 'flipflops': 3, 'ubs': 13, 'ny': 8, 'longisland': 5, 'may20': 14, 'OMRalbum': 2, 'StockholmSyndrome': 4, 'LOT': 15, 'biglietti': 2, 'AIW': 47, 'family': 2, 'Matilda': 12, 'HarryStylesTODAY': 3, 'LouisTomlinson': 50, 'BTC': 12, 'lot': 2, 'HarrysHousePH': 85, 'hs3': 67, 'harrystylesfanart': 2, 'fanart': 3, 'HS3': 52, 'ハリースタイルズ': 8, 'HarryStylesLoveOnTour': 62, 'MiVoto40HarryStyles': 7, 'todayshow': 4, 'LA': 3, 'NewYork': 10, 'HarryStylesONO': 2, 'HARRYSHOUSE': 27, 'Milano': 2, 'comingsoon': 2, 'AppleMusicLive': 5, 'RescueHarry': 5, 'Harrystyles': 4, 'ONOtickets': 2, 'JusticeForJohhnyDepp': 7, 'homophobie': 2, 'Heartstopper': 11, 'zarry': 7, 'Harryshouse': 30, 'Hs3': 3, 'mayadorable': 2, 'styletonic': 2, 'ltwt': 3, 'heineken': 2, 'youarehome': 15, 'LoveOnTour': 50, 'nyc': 11, 'Spotify': 8, 'dolar': 2, 'fitnessgirl': 3, 'GenshinImpact': 3, 'JohnnyDeppvAmberHeard': 7, 'KinnPorscheTheseries': 3, 'HarryStylesManchester': 2, 'harryONO': 11, 'harrywembley': 2, 'vendobiglietti': 25, 'LarryStylinson': 22, 'Larry': 4, 'Tickets': 6, 'Vinyl': 2, 'LoveOnTourKrakow': 10, 'HarryStylesLoveOnTourKrakow': 20, 'canyonmoon': 3, 'asiswas': 2, 'girlcrush': 2, 'golden': 5, 'harryasmyheart': 2, 'UBSArena': 6, 'HSHQ': 10, 'BillieEilish': 4, 'onedirection': 5, '1d': 7, 'lyrics': 2, 'zanelowe': 43, 'vinyl': 9, 'AppleMusic': 9, 'loml': 2, 'Wembley': 18, 'HarrysHouseLATAM': 2, 'harryshouseono': 3, 'BTS_Proof': 5, 'GoIMP': 18, 'VoltaJJ': 16, 'MisterNoFlamemgo': 16, 'loveontourkrakow': 2, 'loveontour2022': 3, 'Harries': 20, 'TaylorSwift': 12, 'TaylorsVersion': 4, 'music': 17, 'LouisTomlinsonWorldTour2022': 18, 'cercobiglietti': 19, 'OneDirection': 38, 'onolondon': 25, 'onlyfans': 2, 'ASITWAS': 12, 'comprobiglietti': 3, 'London': 8, 'harrystylespit': 2, 'harrystylesuk': 3, 'HarryStylesAvecNRJ': 207, 'b': 3, 'boyfriends': 4, 'loveofmylife': 3, 'LGBT': 2, 'lgbt': 5, 'famemma14': 3, 'McDonalds': 4, 'Amici21': 3, 'stockmarketcrash': 3, 'Cryptocrash': 7, 'art': 4, 'NFTCommunity': 4, 'Eurovision': 38, 'TwitterExposed': 3, 'larries': 2, 'myhouse': 3, 'nsfwtwt': 3, 'Sowoozoo': 2, 'JEONJUNGKOOK': 2, 'FreddieGibbs': 2, 'BBMAs': 10, 'quote': 3, 'TheBoys': 5, 'onpoli': 2, 'PGLMajor': 2, 'SEVENTEEN': 3, 'Flames': 7, 'Xbox': 2, 'CBeebiesBedtimeStories': 8, 'ivory': 7, 'Harry_Styles': 3, 'CONCOURS': 6, 'Azovstal': 7, 'Aktien': 2, 'LiamPayne': 5, 'Eurovision2022': 15, 'vendobiglietto': 3, 'RevendaLisboaLoveontour': 9, '1D': 4, 'Trending': 6, 'Terra_Luna': 2, 'iKON': 3, 'SHIB': 2, 'JusticeForJohnnyDepp': 3, 'lunaterra': 2, 'zonauang': 2, 'XRP': 2, 'HalalanResults': 2, 'htafc': 7, 'homemade': 3, 'ineedthis': 2, 'Hearst': 5, 'Bookazine': 5, '海外マガジン': 5, 'magazine': 5, '洋雑誌': 5, 'movie': 5, 'furry': 7, 'CandySugarPop': 7, 'LUNA': 2, 'JohnnyDeppVsAmberHeard': 2, 'BloggersHutRT': 2, 'WritingCommunity': 2, 'writerslift': 2, 'love': 6, 'onenightonlylondon': 8, 'sun': 2, 'BillboardMusicAwards': 2, 'BTS': 4, 'Bitcoin': 9, 'aespa': 6, 'abc730': 8, 'BAC2022': 6, 'btsbestbuyroleplay': 6, 'Cannes2022': 6, 'CNFTProject': 6, 'DeppHeardTrial': 6, 'ENHYPEN': 7, 'Eleksyon2022': 6, 'femboy': 7, 'GyanvapiMosque': 6, 'GAPtheseries': 6, 'ghosthunting': 6, 'Giveaway': 7, 'svp': 6, 'fundraising': 2, 'fundraiser': 2, 'help': 5, 'HSLOT2022': 4, 'KPOPFLEX': 2, 'asitwas': 30, 'larry': 6, 'FacetheSun': 2, 'SoundCloud': 2, 'NewPodcast': 2, 'Buffalo': 4, 'Tucker': 2, 'ReplacementTheory': 2, 'selfawareness': 2, 'TheFuture': 2, 'TurkeyNeck': 4, 'BadDate': 2, 'Comedy': 2, '15orLess': 2, 'Apple': 5, 'etc': 2, 'Gorillaz': 2, 'FAV': 23, 'LongIsland': 8, 'onony': 3, 'HARRYS': 3, 'TikTok': 8, 'shawnmendes': 2, 'hillie': 2, '20DaysOfHarry': 7, 'Daydreaming': 2, 'creatorclash': 2, 'harrie': 4, 'ListeningParty': 3, '4daysillbehome': 2, 'countdown': 2, 'LTWT': 5, 'ubsarena': 15, 'NiallHoran': 5, 'ZaynMalik': 3, 'stuntfree': 2, 'RUMOR': 6, 'HARRYSTYLES': 5, 'ビットコイン': 3, 'ethereum': 3, 'Binance': 3, 'terra': 3, 'luna': 3, 'musica': 2, 'TPWK': 9, 'LTWTChile': 4, 'ReseñaRS': 3, 'StrangerThings4': 4, 'Bridgerton': 4, 'MultiverseOfMadness': 4, 'FineLine': 21, 'concert': 7, 'harrystylespics': 5, 'Fine': 2, 'PayPal': 4, 'luneterra': 2, 'HarryStylesEnAspen': 6, 'SCAM': 2, 'scammers': 2, 'LoveOfMyLife': 2, 'amazon': 2, 'matilda': 2, 'sellingono': 2, 'harryono': 3, 'listeningpartyNapoli': 2, 'NEWARS': 3, 'mondaythoughts': 3, 'MultiVersus': 3, 'Harry': 14, 'jeru': 2, 'Music': 6, 'NewMusic': 5, 'Love': 4, 'LoveStory': 4, 'IGOT7': 2, 'applemusic': 2, 'newyork': 2, 'ZaneLowe': 8, 'Argentina': 2, 'harrystylesclothes': 2, '1dgift': 2, '1dfan': 2, 'etsy': 2, 'harryflipflops': 2, 'harrystylesshoes': 2, 'designerflipflops': 2, 'zayn': 5, 'zarryisreal': 4, 'zarrystylik': 5, 'twitter': 4, 'friendship': 2, 'LOT2022RevendaLisboa': 5, 'eBay': 5, 'HS1': 13, 'instagramdown': 3, 'tpwk': 4, 'watermelonsugar': 2, 'Love0nTour2022': 2, 'EclipseLunar': 2, 'album': 2, 'styles': 2, 'hs1': 8, 'harrymanip': 2, 'Crypto': 4, 'harrychella': 4, 'America': 4, 'CapeTown': 2, 'harryedwardstyles': 3, 'harrystylesvzla': 2, 'harrystylesvenezuela': 2, 'harrystylesliveontour': 2, 'hsdailyfeature': 2, 'GiantsSony': 19, 'MyPoliceman': 3, 'preorder': 2, '911LoneStar': 2, 'SPOILER': 4, 'newalbum': 5, 'Amsterdam': 2, 'matury2022': 3, 'tweetme': 6, 'spoilers': 2, 'Berlin': 2, 'REVIEW': 2, 'lightsup': 2, 'kiwi': 2, 'signofthetimes': 2, 'competition': 2, 'win': 2, 'newmusic2022': 3, 'Fineline': 3, 'AnjaliArora': 2, 'EUROVISION': 7, 'KAI': 4, 'NYR': 2, 'OliviaWilde': 4, 'holivia': 3, 'Coachella2022': 3, '5SOS5': 7, 'thr': 2, 'losangeles': 2, 'hollywood': 2, 'rollingstone': 2, 'billboardnews': 2, 'popmusic': 3, 'HeartstopperNetflix': 2, 'buynow': 2, 'HarryHouseLive': 6, 'PasaporteMatch': 2, 'butinacoolway': 2, 'Wanda': 2, 'Ecuador': 3, 'Verga': 3, 'Mameluco': 2, 'IVE': 3, 'JIN': 3, 'lunacoin': 4, 'OC': 3, 'Dreamcatcher': 3, 'ULTIMAHORA': 3, 'URGENTE': 3, 'Guayaquil': 2, 'Quito': 2, 'Cuenca': 2, 'RT': 4, 'Sexo': 2, 'Chanel': 2, 'SiguemeYTeSigo': 4, 'Italy': 2, 'As': 2, 'moots': 2, 'foryoupage': 5, 'buckybarnes': 4, 'MyBTSTracks': 5, 'DoctorStrange': 4, 'ThisIsUs': 2, 'KendrickLamar': 4, 'sundayvibes': 2, 'TheWilds': 2, 'TheNorthman': 2, 'TheKardashians': 2, 'FACupFinal': 12, 'Juliette': 2, 'london': 8, 'manchester': 4, 'EmiratesFACup': 12, 'iHeartRadio': 3, 'Loveontour2022': 2, 'GOT7': 4, 'LOTParis': 3, 'MostRequestedLive': 2, 'leak': 3, 'NowPlaying': 3, 'loveislove': 7, 'ticketmaster': 10, 'eurovisiongr': 13, 'ESC2022': 13, 'Twitter': 2, 'dreamchat': 12, 'runningmate': 11, 'EmiratesFACupfinal': 11, 'UK': 13, 'EurovisionRTVE': 11, 'loveontouramsterdam': 2, 'loveontourantwerpen': 2, 'WembleyStadium': 3, 'cercobiglietto': 2, 'MUNI': 2, 'HARRY': 3, 'larrystylinson': 3, 'HarrysHouseNights': 5, 'Israel': 2, 'GoogleIO': 3, 'Telegram': 2, 'HSIQ': 59, 'conangraywarsaw': 3, 'SoldOut': 3, 'falling': 4, 'protective': 2, 'funny': 2, '1989TaylorsVersion': 4, 'Swifties': 4, 'Harriers': 2, 'harrystylestoday': 2, 'Ticketmaster': 3, 'sellingtickets': 2, 'instagram': 2, 'May20th': 2, 'HarrysHouseBR': 2, 'Musica': 2, 'harrystyle': 2, 'Pleasing': 2, 'TaylorNation': 2, 'zaddy': 6, 'BehindTheScenes': 3, 'loveontourwembley': 2, 'harrystylestour': 2, 'HarryStyleAvecNRJ': 7, 'Hamburg': 2, 'brixton': 4, 'HarryStylesMemes': 5, 'holiviamemes': 5, 'ticketscammer': 2, 'scambiobiglietti': 3, 'LOUIES': 4, 'StrangeNewWorlds': 2, 'May20': 4, 'Golden': 6, 'HS2': 4, 'WatermelonSugar': 5, 'AdoreYou': 5, 'LightsUp': 4, 'Cherry': 4, 'Falling': 5, 'ToBeSoLonely': 4, 'She': 4, 'Styles': 5, 'Sunflower': 4, '5YearsOfHS1': 6, 'NRJRadioStream': 2, 'Manchester': 2, 'LarryIsReal': 2, 'ofmd': 2, 'lol': 2, 'fyp': 5, 'ticketscam': 4, 'hslot2022': 3, 'harrystylesmadrid': 2, 'thursdayvibes': 3, 'latenighttalking': 2, 'hsiq': 11, 'harrystylesupdates': 2, 'fypシ': 4, 'foryourpage': 4, 'fypage': 4, 'daddy': 4, 'lgbtqia': 4, 'lgbtqi': 4, 'lgbtq': 4, 'formepage': 4, 'FORDfortheBuilders': 4, 'LIKEABOMBSHELL': 4, 'boyfriend': 4, 'queer': 4, 'Claromúsica': 2, 'Debut': 2, 'HarryStylestickets': 2, 'HarryStylesavecNRJ': 2, 'pop': 2, 'aiw': 4, 'bringitback': 2, 'blockchain': 2, 'JAEMIN': 2, 'harrystylesavecnrj': 2, '5SOS': 2, 'discoball': 7, 'bbc': 9, 'sellingonotickets': 3, 'sebastianmoy': 2, 'Sebastian': 2, 'moy': 2, 'moybrothers': 2, 'olivermoy': 2, 'oliver': 2, 'LoveOnTourLA': 2, 'Biblia': 3, 'Jesus': 3, 'RicardoClaurePeñaloza': 3, 'SinfoniasCelestiales': 3, 'Profecias': 3, '34añosDeHistoria': 3, 'SalvarElAlmaHastaDelUltimoHombre': 3, 'ONEUS': 3, 'HARRYSHOUSESLEEPOVER': 2, 'EnDesarrollo': 2, 'SinLuz': 2, 'BuenMiercoles': 2, 'MiercolesDeGanarSeguidores': 2, 'TenesUnAirePhilco': 2, 'matura2022': 2, 'ThisLoveTaylorVersion': 2, 'burla': 2, 'm2o': 2, 'hsono': 3, 'onopresale': 2, 'harrystylesedit': 2, 'LondonONO': 2, 'Video': 2, 'harrystylesfan': 2}\n"
     ]
    }
   ],
   "source": [
    "hasthags_to_save = {}\n",
    "\n",
    "for c in c_hashtags:\n",
    "    if c_hashtags[c] > 1:\n",
    "        hasthags_to_save[c] = c_hashtags[c]\n",
    "\n",
    "print(hasthags_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(hasthags_to_save, open('hashtags_1705.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = []\n",
    "\n",
    "for t in tweets_extended:\n",
    "    try:\n",
    "        tweets = t['entities'].get('mentions')\n",
    "        for x in tweets:\n",
    "            mentions.append(x['username'])\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3115\n"
     ]
    }
   ],
   "source": [
    "print(len(mentions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a contare le menzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mentions</th>\n",
       "      <th>Tot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry_Styles</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Z100NewYork</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HSHQ</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NRJhitmusiconly</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sonymusicph</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zanelowe</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>youarehome</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sottluvr</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sweetapoIIo</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>adorefiIm</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>milfaylor</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>iHeartRadio</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AppleMusic</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sunflowerfrey</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>twocinemas</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hsdaily</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>team_world</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sonymusicbrasil</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>vulture</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sonymusicmexico</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Mentions  Tot\n",
       "0      Harry_Styles  661\n",
       "1       Z100NewYork  243\n",
       "2              HSHQ  188\n",
       "3   NRJhitmusiconly  128\n",
       "4       sonymusicph   74\n",
       "5          zanelowe   68\n",
       "6        youarehome   49\n",
       "7          sottluvr   45\n",
       "8       sweetapoIIo   42\n",
       "9         adorefiIm   36\n",
       "10        milfaylor   32\n",
       "11      iHeartRadio   30\n",
       "12       AppleMusic   28\n",
       "13    sunflowerfrey   25\n",
       "14       twocinemas   24\n",
       "15          hsdaily   23\n",
       "16       team_world   16\n",
       "17  sonymusicbrasil   15\n",
       "18          vulture   15\n",
       "19  sonymusicmexico   14"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_mentions = Counter(mentions)\n",
    "\n",
    "pd.DataFrame(\n",
    "    c_mentions.most_common()[:20],\n",
    "    columns = ['Mentions', 'Tot']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esportiamo tutto su un file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = [\"Mentions\", \"Tot\"]\n",
    "rows = []\n",
    "for v in c_mentions.most_common()[:20]:\n",
    "    row = []\n",
    "    row.append(v[0])\n",
    "    row.append(str(v[1]))\n",
    "    rows.append(row)\n",
    "    \n",
    "with open('tot_mentions_1705.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(head)\n",
    "    writer.writerows(rows)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a stampare le informazioni sugli utenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_users = {}\n",
    "\n",
    "for t in tweets_extended:\n",
    "\n",
    "    u = t['author']\n",
    "    \n",
    "    id_user = u['id']\n",
    "    username = u['username']\n",
    "    isVerified = u['verified']\n",
    "        \n",
    "    user = u['public_metrics']\n",
    "    followers = user['followers_count']\n",
    "    following = user['following_count']\n",
    "    tweet_count =  user['tweet_count']\n",
    "    \n",
    "    try:\n",
    "        location = u['location']\n",
    "    except:\n",
    "        location = \"\"\n",
    "        \n",
    "    list_users[id_user] = [\n",
    "        id_user,\n",
    "        username,\n",
    "        isVerified,\n",
    "        followers,\n",
    "        following,\n",
    "        tweet_count,\n",
    "        location.replace(\"\\n\", \" \")\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4952"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(list_users , open('users_1705.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = [\"Id User\", \"Username\", \"Verified\", \"Followers\", \"Following\", \"Tweet Count\", \"Location\"]\n",
    "\n",
    "with open('users_1705.csv', 'w', newline='', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(head)\n",
    "    writer.writerows(list_users.values())    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "fcc24bee83e622ede442be937b3581127ef7fd4bca8dd9a049c9af0d6e60586f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
